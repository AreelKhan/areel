---
title: "Jailbreaking LLMs: Competing Objectives and Mismatched Generalization"
description: "Takeaways and thoughts after reading \"Jailbroken: How Does LLM Safety Training Fail\""
date: "2025-08-9"
tags: ['llm-jailbreaking']
---


- Go over what mistmatched generalizatoin is
- what competing objectices are
- a little bit of intro about the field, how llms are safety aligned



questions to ask:
- in depth, how are llms even safety aligned? 
- philospahically, should llm providers be taking responsibility for the content they return? does a someone making a knife have to take responsibility for the knife being used to kill someone? or does someone making a brick have to take responsibility for the brick being used to kill someone?
or does someone selling a computer have to take responsibility for the computer being used to hack? why should LLM providers be taking responsibility for the content they return?

Im not saying they shouldnt, im just saying it is intereating that these tech people have to take the responsiblity of deciding what information is safe to access and what is not. at what point does llm safety become censorship?