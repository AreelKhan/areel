---
title: "LLMs only predict the next word in a sentence"
description: "A hand-wavy explanation for how LLMs generate responses"
date: "2025-08-09"
tags: ['llms']
---


### Why I am writing this
A few weeks ago, while sitting around a campfire (and being destroyed by mosquitos), my friends and I attempted to define what it means to "think".

I began contemplating if what an LLM does could be called thinking. Although I would've loved to discuss that with my friends, they are not familiar with how an LLM works. And I certainly did not want to go on a tangent about embeddings and autoregression.

If I were, however, to try to explain to them what an LLM does, I'd go about it something like this.


### Predicting the next word
LLMs (like ChatGPT) can simply be thought of as a bot that predicts what the most likely next word in a sentence is. That's really it.

An example:

Let's say you ask ChatGPT how a car works. The LLM will see:

```
Question: Explain to me how a car works. Response:
```

The "Question:" and "Response:" labels are added by the system between you and the LLM.

The LLM now predicts the most likely next word in this sequence. It might predict "A":

```
Question: Explain to me how a car works. Response: A
```

Again, it predicts the next word. It predicts "car":

```
Question: Explain to me how a car works. Response: A car
```

Then it predicts "works":

```
Question: Explain to me how a car works. Response: A car works
```

And so on. It will continue to predict the most likely next word, until the most likely next word is "no word", in other words, the end of the sentence.

So LLMs are actually loops[\*](#footnote-1):
- you put your question in the LLM
- out comes our your question and as well as the first word of the answer
- these go back into the LLM
- and then out comes your question, the first word, AND the second word of the answer
- and so on...




So now think to yourself, are LLMs "thinking"? I don't know, I don't even know what thinking means. It is, however, fascinating to think how such a stupid process can product such intelligent responses. It's admirable.

### Not __just__ the next word

LLMs don't predict __just__ the most likely next word. They also predict the second most likely next word, and third most and so on. You can think of it as a ranking. The LLM is basically saying, given this sentence, here is the next word that makes most sense, heres the one that makes second most sense, and so on. In technical terms, this is a probability distribution over all possible words in the LLM's vocabulary. We'll come back to vocabulary later.

When the LLM responds with its ranking, its the system that sits between you and the LLM that chooses which word to actually use. If you configure that system to pick the most likely word, then you would get (in a sense[\*\*](#footnote-2)) the most likely response to your question. However, you could configure it to instead pick one of the top three most likely words, in this case you'd get a more different or "unpredictable" response. The "temperature" of the LLM is a setting that lets you control how much randomness you want in the response. Here is an example:



Temperature: 0
Input: Describe a sunset in one sentence.
Output: The sun sets slowly, painting the sky in shades of orange and pink.

Temperature: 0.5
Input: Describe a sunset in one sentence.
Output: The sun dips below the horizon, scattering warm oranges, pinks, and faint purples across the clouds.

Temperature: 1.0
Input: Describe a sunset in one sentence.
Output: The sky bursts into swirling streaks of crimson and gold as the fading sun kisses the sea.

Temperature: 1.5
Input: Describe a sunset in one sentence.
Output: The horizon melts into a chaotic blaze of molten gold and violet, as if the sky is dreaming in fire.





Also LLMs don't actually predict the most likely word, but instead the most likely token. Tokens are like subwords, you can kind of think of them as syllables. So when you really think about it, LLMs are just predicting the most likely next 1-3 letter syllable. Crazy how they can look so intelligent for such a "simple" process. The reasons tokens are used instead of words are numerous. But I think the main reason is to reduce the size of the vocabulary the LLM has to learn. For the LLM to predict the next word, it has to know all the possible words in english, which is almost 200k words, thats a lot to remember. Whereas if you ask the LLM to predict the most likely next syllable, well theres only about 10k syllables, and it only needs to know those.



<a id="footnote-1"></a> \* It is the system that sits between you and the LLM that handles the loop. Remember, the LLM literally only predicts the next word. It is not capable of taking the sentence and giving it back to itself in a loop. Hence, there needs to be a system that feeds the LLM the sentence again and again until the LLM says the sentence is complete.

<a id="footnote-2"></a> \*\* Not for the unintiated: this depends on what you mean by "most likely response". If it is one where the product of the next predicted individual tokens is maximum (ie product of P(next token = token you picked | previous tokens)), then yes, LLMs with temperature 0 return the most likely sentence. Kind of like a greedy algo, just pick the most likely right now. But if you look at it more dynamically, where it is possible that the most likely next token, may actually lead you down a path where every token that follows has a very low probability, then the greedy llm approach does not apply. this might be more akin to how humans think. we dont predict the next word in our sentence. we think of general ideas (ie entire sentences) at once, and then spit them out.


<a id="footnote-3"></a> \*\*\*. In fact, if my understanding is correct, this is the only source of randomness in the LLM. If you always picked the most likely word, you'd get the same response every time.